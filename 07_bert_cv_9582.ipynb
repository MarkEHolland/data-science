{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 1: Intent Referral - using Bert embeddings & ML with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DC code 9582 (unexpected customer delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "base_path = Path(\"/home/jupyter/deemed_consent\")\n",
    "data_dir = Path(\"data\")\n",
    "data_path = base_path / data_dir / \"deemed_consent_preprocessed_1 1.csv\"\n",
    "embedding_path = base_path / data_dir / \"embeddings_9582.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               obs  obs_pct\n",
      "REASON_CODE                \n",
      "2002         36737     31.2\n",
      "3007         16576     14.1\n",
      "9582         14073     11.9\n",
      "2047          7392      6.3\n",
      "3005          6025      5.1\n",
      "2064          4483      3.8\n",
      "2042          4457      3.8\n",
      "3010          3416      2.9\n",
      "3011          3415      2.9\n",
      "3001          3133      2.7\n"
     ]
    }
   ],
   "source": [
    "data_df=pd.read_csv(data_path)\n",
    "top_10_reason_codes=pd.DataFrame({\"obs\":data_df['REASON_CODE'].value_counts().head(10), \"obs_pct\": (100 * data_df['REASON_CODE'].value_counts().head(10) / len(data_df)).round(1)})\n",
    "print(top_10_reason_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select the dc code to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc_code = 9582 # top3: 2002, 3007, or, 9582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reduced_df= data_df[data_df['REASON_CODE']==dc_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with class 1: 2587\n",
      "Number of rows with class 0: 11486\n"
     ]
    }
   ],
   "source": [
    "#training_df=reason_df[reason_df['training_data'] == 1]\n",
    "count_class_1 = (reduced_df['draft_referral_flag'] == 1).sum()\n",
    "print(\"Number of rows with class 1:\", count_class_1)\n",
    "count_class_0 = (reduced_df['draft_referral_flag'] == 0).sum()\n",
    "print(\"Number of rows with class 0:\", count_class_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of intents referred back: 0.18\n"
     ]
    }
   ],
   "source": [
    "ratio = count_class_0 / count_class_1\n",
    "print(f\"ratio of intents referred back: {round(ratio,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess and clean the delay notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# look for missing data\n",
    "print(reduced_df['DELAY_NOTES_CLEANED'].isna().sum())\n",
    "null_count = reduced_df['DELAY_NOTES_CLEANED'].isna().sum()\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values: 0\n",
      "Empty strings: 0\n",
      "Valid notes: 14073\n"
     ]
    }
   ],
   "source": [
    "# Convert to string and analyze\n",
    "valid_notes = reduced_df['DELAY_NOTES_CLEANED'].fillna('')\n",
    "empty_strings = (valid_notes.str.strip() == '').sum()\n",
    "total_rows = len(reduced_df)\n",
    "print(f\"Null values: {null_count}\")\n",
    "print(f\"Empty strings: {empty_strings}\")\n",
    "print(f\"Valid notes: {total_rows - null_count - empty_strings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_safely(text):\n",
    "    \"\"\"\n",
    "    Safely counts words in text, handling null values and non-string inputs\n",
    "    \"\"\"\n",
    "    if pd.isna(text):  # Handle null values\n",
    "        return 0\n",
    "    try:\n",
    "        return len(str(text).split())\n",
    "    except AttributeError:\n",
    "        print(f\"Unexpected data type: {type(text)}\")\n",
    "        return 0\n",
    "\n",
    "# word counting code\n",
    "no_eng_words_arr = []\n",
    "for text in reduced_df['DELAY_NOTES_CLEANED']:\n",
    "    no_words = count_words_safely(text)\n",
    "    no_eng_words_arr.append(no_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Nan with \"\"\n",
    "reduced_df['DELAY_NOTES_CLEANED'] = reduced_df['DELAY_NOTES_CLEANED'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean words: 79.8, max 269, min 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean words: {round(np.mean(no_eng_words_arr),1)}, max {np.max(no_eng_words_arr)}, min {np.min(no_eng_words_arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load bert-base-uncased\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/jupyter/deemed_consent/bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"/home/jupyter/deemed_consent/bert-base-uncased\")\n",
    "\n",
    "def get_bert_embeddings(texts, max_length=250):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding='max_length', truncation=True, \n",
    "                          max_length=max_length, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get CLS token embeddings\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = reduced_df['DELAY_NOTES_CLEANED']\n",
    "y = reduced_df['draft_referral_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert X data to lists\n",
    "X_list = X.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [1:25:03<00:00, 11.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate BERT embeddings\n",
    "X_emb = get_bert_embeddings(X_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to a .npy file\n",
    "np.save(embedding_path, X_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load embeddings (i.e. short cut if embeddings already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the embeddings back\n",
    "X_loaded_emb = np.load(embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross validate with xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(scale_pos_weight=ratio, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(model, X_loaded_emb, y, cv=cv, scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'neg_log_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([15.63823891,  9.09886622,  9.09165359,  9.45264077,  9.82465291]), 'score_time': array([0.05791235, 0.03008819, 0.03036141, 0.02866292, 0.0289371 ]), 'test_accuracy': array([0.81740675, 0.8348135 , 0.81882771, 0.82658138, 0.83759773]), 'test_precision': array([0.55725191, 0.65354331, 0.62162162, 0.61290323, 0.67741935]), 'test_recall': array([0.13799622, 0.16468254, 0.12849162, 0.14728682, 0.16766467]), 'test_f1': array([0.22121212, 0.26307448, 0.21296296, 0.2375    , 0.2688    ]), 'test_roc_auc': array([0.75783184, 0.78902918, 0.79451126, 0.78457633, 0.78314491]), 'test_neg_log_loss': array([-0.60130259, -0.51430726, -0.55105206, -0.54394054, -0.52492579])}\n"
     ]
    }
   ],
   "source": [
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.827\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean accuracy: {round(cv_results['test_accuracy'].mean(),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean precision: 0.625\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean precision: {round(cv_results['test_precision'].mean(),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean recall: 0.149\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean recall: {round(cv_results['test_recall'].mean(),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
